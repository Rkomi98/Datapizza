{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Confronto Pratico: Modelli BASE vs INSTRUCT\n",
        "\n",
        "## Obiettivo\n",
        "Questo notebook dimostra le differenze pratiche tra modelli **BASE** (pretrained) e **INSTRUCT** (instruction-tuned) attraverso:\n",
        "- Confronto di tecniche di prompting\n",
        "- Batteria di 5 task diversi\n",
        "- Analisi di safety/refusal behavior\n",
        "- Metriche di performance e accuratezza\n",
        "\n",
        "**Modelli utilizzati:**\n",
        "- BASE: Modello pre-addestrato (completamento di testo)\n",
        "- INSTRUCT: Modello fine-tuned per seguire istruzioni\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup e Installazione Dipendenze\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installazione delle dipendenze necessarie\n",
        "%pip install transformers>=4.42 accelerate torch datasets evaluate sacrebleu rouge-score tiktoken matplotlib pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import delle librerie\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from rouge_score import rouge_scorer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Impostazione del seed per riproducibilit√†\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"Setup completato!\")\n",
        "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configurazione Modelli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ CONFIGURAZIONE ============\n",
        "CONFIG = {\n",
        "    'provider': 'hf',\n",
        "    'base_model_id': 'Qwen/Qwen2.5-7B',  # Modello BASE\n",
        "    'instruct_model_id': 'Qwen/Qwen2.5-7B-Instruct',  # Modello INSTRUCT\n",
        "    'dtype': 'auto',\n",
        "    'device_map': 'auto',\n",
        "    'max_new_tokens': 256,\n",
        "    'temperature': 0.2,\n",
        "    'load_in_8bit': False,  # Cambia a True se hai problemi di memoria\n",
        "    'load_in_4bit': False,  # Alternativa per memoria molto limitata\n",
        "    'trust_remote_code': False\n",
        "}\n",
        "\n",
        "print(\"Configurazione:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caricamento Modelli e Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Caricamento del modello BASE\n",
        "print(\"Caricamento modello BASE...\")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CONFIG['base_model_id'],\n",
        "    trust_remote_code=CONFIG['trust_remote_code']\n",
        ")\n",
        "\n",
        "load_kwargs = {\n",
        "    'device_map': CONFIG['device_map'],\n",
        "    'torch_dtype': CONFIG['dtype'],\n",
        "    'trust_remote_code': CONFIG['trust_remote_code']\n",
        "}\n",
        "\n",
        "if CONFIG['load_in_8bit']:\n",
        "    load_kwargs['load_in_8bit'] = True\n",
        "elif CONFIG['load_in_4bit']:\n",
        "    load_kwargs['load_in_4bit'] = True\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG['base_model_id'],\n",
        "    **load_kwargs\n",
        ")\n",
        "\n",
        "# Aggiunta pad_token se mancante\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "print(f\"Modello BASE caricato: {CONFIG['base_model_id']}\")\n",
        "print(f\"Vocab size: {base_tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Caricamento del modello INSTRUCT\n",
        "print(\"Caricamento modello INSTRUCT...\")\n",
        "instruct_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CONFIG['instruct_model_id'],\n",
        "    trust_remote_code=CONFIG['trust_remote_code']\n",
        ")\n",
        "\n",
        "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG['instruct_model_id'],\n",
        "    **load_kwargs\n",
        ")\n",
        "\n",
        "if instruct_tokenizer.pad_token is None:\n",
        "    instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n",
        "\n",
        "print(f\"Modello INSTRUCT caricato: {CONFIG['instruct_model_id']}\")\n",
        "print(f\"Vocab size: {instruct_tokenizer.vocab_size}\")\n",
        "print(f\"Chat template disponibile: {instruct_tokenizer.chat_template is not None}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funzione di Generazione Unificata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt_or_messages, is_chat=False):\n",
        "    \"\"\"\n",
        "    Funzione unificata per generare testo con modelli BASE e INSTRUCT\n",
        "    \n",
        "    Args:\n",
        "        model: Modello HuggingFace\n",
        "        tokenizer: Tokenizer corrispondente\n",
        "        prompt_or_messages: String (BASE) o lista di dict (INSTRUCT)\n",
        "        is_chat: Se True, usa chat template (per INSTRUCT)\n",
        "    \n",
        "    Returns:\n",
        "        dict con: text, num_input_tokens, num_output_tokens, latency_s\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if is_chat and isinstance(prompt_or_messages, list):\n",
        "        # Modalit√† chat (INSTRUCT)\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            prompt_or_messages,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False\n",
        "        )\n",
        "    else:\n",
        "        # Modalit√† plain text (BASE)\n",
        "        formatted_prompt = prompt_or_messages\n",
        "    \n",
        "    # Tokenizzazione\n",
        "    inputs = tokenizer(\n",
        "        formatted_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    ).to(model.device)\n",
        "    \n",
        "    num_input_tokens = inputs.input_ids.shape[1]\n",
        "    \n",
        "    # Generazione\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=CONFIG['max_new_tokens'],\n",
        "            temperature=CONFIG['temperature'],\n",
        "            do_sample=True if CONFIG['temperature'] > 0 else False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decodifica solo della parte generata\n",
        "    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    num_output_tokens = len(generated_tokens)\n",
        "    latency_s = time.time() - start_time\n",
        "    \n",
        "    return {\n",
        "        'text': generated_text.strip(),\n",
        "        'num_input_tokens': num_input_tokens,\n",
        "        'num_output_tokens': num_output_tokens,\n",
        "        'latency_s': latency_s\n",
        "    }\n",
        "\n",
        "print(\"Funzione di generazione definita!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 1: Confronto Prompting vs Chat Template\n",
        "\n",
        "Mostriamo come lo stesso compito viene gestito diversamente dai due tipi di modello.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task di esempio: spiegazione del colore del cielo\n",
        "task_prompt = \"Spiega in 3 punti perch√© il cielo √® blu e termina con una frase conclusiva in grassetto.\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CONFRONTO: BASE vs INSTRUCT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============ MODELLO BASE - Zero-shot ============\n",
        "print(\"\\nüî∏ MODELLO BASE (Zero-shot):\")\n",
        "base_prompt_zero = f\"{task_prompt}\\n\\nRisposta:\"\n",
        "result_base_zero = generate(base_model, base_tokenizer, base_prompt_zero, is_chat=False)\n",
        "\n",
        "print(f\"Input tokens: {result_base_zero['num_input_tokens']}\")\n",
        "print(f\"Output tokens: {result_base_zero['num_output_tokens']}\")\n",
        "print(f\"Latency: {result_base_zero['latency_s']:.2f}s\")\n",
        "print(f\"Risposta:\\n{result_base_zero['text']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ MODELLO INSTRUCT - Chat Template ============\n",
        "print(\"\\nüî∏ MODELLO INSTRUCT (Chat Template):\")\n",
        "instruct_messages = [\n",
        "    {\n",
        "        \"role\": \"system\", \n",
        "        \"content\": \"Sei un assistente che risponde sempre in formato puntato numerato. Ogni risposta deve terminare con una frase conclusiva in grassetto.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\", \n",
        "        \"content\": task_prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "result_instruct = generate(instruct_model, instruct_tokenizer, instruct_messages, is_chat=True)\n",
        "\n",
        "print(f\"Input tokens: {result_instruct['num_input_tokens']}\")\n",
        "print(f\"Output tokens: {result_instruct['num_output_tokens']}\")\n",
        "print(f\"Latency: {result_instruct['latency_s']:.2f}s\")\n",
        "print(f\"Risposta:\\n{result_instruct['text']}\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"OSSERVAZIONI SEZIONE 1:\")\n",
        "print(\"‚Ä¢ Il modello BASE necessita di pi√π contesto (few-shot) per seguire il formato\")\n",
        "print(\"‚Ä¢ Il modello INSTRUCT segue immediatamente le istruzioni nel system prompt\")\n",
        "print(\"‚Ä¢ La latency pu√≤ variare significativamente tra i due approcci\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Note di Utilizzo\n",
        "\n",
        "## Prossimi Passi\n",
        "Il notebook ora √® pronto per l'uso! Per estendere l'analisi, puoi aggiungere:\n",
        "\n",
        "1. **Batteria di 5 Task di Valutazione**: QA, Sintesi, JSON, Aritmetica, Codice Python\n",
        "2. **Test di Safety**: Confronto comportamenti di refusal\n",
        "3. **Analisi Performance**: Visualizzazioni e metriche aggregate\n",
        "4. **Fine-tuning Demo**: Setup LoRA per personalizzazione\n",
        "\n",
        "## Requisiti Sistema\n",
        "- **GPU**: Raccomandato almeno 16GB VRAM per i modelli 7B\n",
        "- **RAM**: Minimo 32GB di RAM di sistema\n",
        "- **Storage**: Circa 15GB per i modelli scaricati\n",
        "\n",
        "## Configurazione Memoria\n",
        "Se hai limitazioni di memoria, modifica la sezione CONFIG:\n",
        "- `load_in_8bit: True` per ridurre l'uso di VRAM\n",
        "- `load_in_4bit: True` per uso su GPU pi√π piccole\n",
        "- Riduci `max_new_tokens` per generazioni pi√π veloci\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
